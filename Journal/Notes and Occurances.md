# Thoughts and Occurances

## With Respect to the Consequences of Fat Tails in Statistical Consequences

It may be that the very infrastructure on which we model may be wrong, and that far too many normality and modelability assumptions are made, and this may present a crack that has yet to be discovered, and requires only such a tail event to occur for it to blow up on us. Though it is always possible that rather everything is fine.

If not however it may reveal to what more things are certain to be beyond our grasp at the moment, and could potentially focus efforts or just make use of data science, where such use is good.  

In addition, it may very well present oppurtunities for new investing practices, such as effectively short selling hype, the way value investors buy statistcally profitable bundles of cheap securities. Perhaps they may even reveal things about the others such as growth about value and value about growth, fragility about value and value about fragility. As well it may present oppurtunities to reconsider a host of other decisions entirely built on on flawed foundations.

It appears most RL and other neural network techniques also tend to be dependant on assumptions of normality and tending toward normality sufficient a large sample size.

If this is the case then perhaps an RL model can be devolped such that it can more quickly determine what kind of function, or tail coefficient a distribution should be generated with and thus behave accordingly.

Watching for large deviations within a collection of data i.e. outliers of some consistent but low frequency is likely the best way to find if the domain is fat tailed, and so dangerous for things like an RL model to learn/predict on. 

However the solution cannot be to just simply do nothing, even for us as people we have to learn to take appropriately hedged bets meaning thurough understandings of risk, which itself may be something that might be difficult to analyse from a statistical perspective i.e with use of past data, though it may not be. This may be one of the interesting ideas to investigate.

A point about Standard Deviation and beyond, for any 2 vectors assuming their L1/mean is the same if the dispersion is wider for one than another, say [1,1] and [2/3,4/3] the l2/STD is going to be larger, and the difference will increase explosively the larger an l norm you take. 

Hence Taleb advises using the MAD for both stability,and communicability of an idea of average deviation, though even still you can't make much of a prediction with MAD, it's just more useful to describe what it ddescribes. So I don't know how we would solve the quandry of what action/exposure to take for an RL prgram, except for that it must be a hedged one in potentially fat tailed situations. 

Infinite variance models based on thing like Cauchy Stable or 2 sided Pareto or Student T with a small tail exponenet can all simmulate fat tailed conditions, although they may all have different shapes, the tail exponent chosen for a Student T, does appear to be pretty robust, so it can occasionaly be useful to see grey swans assuming the data fit reasonably well. This means we can find a reasonable price for the insurance of a situation, as well as how to intelligently take exposure so we don't go bust, perhaps that idea can be trnaslated over to the world of RL or just ML in general.

## With Repect to Work and Learning

It is not unreasonable to approach any obstacle of life or work as things to be tackled by a process of refined reasearch, with hypothesis, observations, analysis, and what that could possibly tell you about the domain with which you're interacting.

I find more and more that where things, fail, break, and don't break is substantially more important to understand then where they do. I suppose this is encapsulated in the 2 phrases, "It ain't what you know, it's what you know for sure but just aint so." and "I want to know where I'll die so I'll never go there". I think possibly just following those 2 ideas and constantly looking for where failure is likely and avoiding them leaves just the success to take care of itself.