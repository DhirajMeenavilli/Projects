# Thoughts and Occurances
## General Thoughts

Humans as a species really are much like a single child alone in space it seems, but that means we too are maturing, realising we are not perhaps the center of the story, but rather just another component and part of the universe that there are things we in fact don't and maybe will never know and that we too can die. It's interesting to note some parralel between the growth of a human child from gestation through maturity, and our species as a whole.

Given that life in itself is just like a game, or more accurately we make it as such, we can think of improving in a certain place as grinding like in WoW, and in the same way with being able to understand runes, exploring the lore, getting better at combat, becoming a better merchant, going on quests, and just sandboxing is all part of the fun and the expirience, so too is it the case for any discipline or field. Watching streamers do random things is also part of the expirience, it also speaks to a lot of how to structure the way you play because rarely do you want to be frustrated or irritated while playing, but you do want to get better and be the best.

I'm surrounded by magic yet fundamentally unable to harness it, because the method of instruction clashes with my mind, the purposes for which I want to harness are unable to satiate my need or food and shelter, and most fundamentally I can't seem to find what I really do want to apply it to/have a metric that allows me to be satisfied and not need more and more info about everything and a want to do everything.

I got so caught up in showing people in positions of power that I'm interested and in love with the things that I'm interested and in love with, that I just kind of forgot to be interested and fall deeper in love with those things. 

Envy (The desire of what others have) and Ego (The desire for what you don't have) it appears to me to be the greatest sources of noise, which if reduced or eliminated allow one to focus deeply on the signals of expierence itself to enjoy the universe and work, and proverbially paint with all the colors of the wind.

Oftentimes the greatest self impedement is the reflection on cohesion during the proccess of creation or construction. I find it is much better to simply write, paint, work, etc. in flow creating what you need as you need it, and then once it's all done cohesively wrapping it up into a nice bundle via comments, logic, flowcharting, etc.

Try and resolve situations favorably (this generally means everyone is happier), if not possible and not life threatning, just split, that's what good intuition tells us to do.

## With Respect to the Consequences of Fat Tails in Statistical Statements

It may be that the very infrastructure on which we model may be wrong, and that far too many normality and modelability assumptions are made, and this may present a crack that has yet to be discovered, and requires only such a tail event to occur for it to blow up on us. Though it is always possible that rather everything is fine.

If not however it may reveal to what more things are certain to be beyond our grasp at the moment, and could potentially focus efforts or just make use of data science, where such use is good.  

In addition, it may very well present oppurtunities for new investing practices, such as effectively short selling hype, the way value investors buy statistcally profitable bundles of cheap securities. Perhaps they may even reveal things about the others such as growth about value and value about growth, fragility about value and value about fragility. As well it may present oppurtunities to reconsider a host of other decisions entirely built on on flawed foundations.

It appears most RL and other neural network techniques also tend to be dependant on assumptions of normality and tending toward normality sufficient a large sample size.

If this is the case then perhaps an RL model can be devolped such that it can more quickly determine what kind of function, or tail coefficient a distribution should be generated with and thus behave accordingly.

Watching for large deviations within a collection of data i.e. outliers of some consistent but low frequency is likely the best way to find if the domain is fat tailed, and so dangerous for things like an RL model to learn/predict on. 

However the solution cannot be to just simply do nothing, even for us as people we have to learn to take appropriately hedged bets meaning thurough understandings of risk, which itself may be something that might be difficult to analyse from a statistical perspective i.e with use of past data, though it may not be. This may be one of the interesting ideas to investigate.

A point about Standard Deviation and beyond, for any 2 vectors assuming their L1/mean is the same if the dispersion is wider for one than another, say [1,1] and [2/3,4/3] the l2/STD is going to be larger, and the difference will increase explosively the larger an l norm you take. w

Hence Taleb advises using the MAD for both stability,and communicability of an idea of average deviation, though even still you can't make much of a prediction with MAD, it's just more useful to describe what it ddescribes. So I don't know how we would solve the quandry of what action/exposure to take for an RL prgram, except for that it must be a hedged one in potentially fat tailed situations. 

Infinite variance models based on thing like Cauchy Stable or 2 sided Pareto or Student T with a small tail exponenet can all simmulate fat tailed conditions, although they may all have different shapes, the tail exponent chosen for a Student T, does appear to be pretty robust, so it can occasionaly be useful to see grey swans assuming the data fit reasonably well. This means we can find a reasonable price for the insurance of a situation, as well as how to intelligently take exposure so we don't go bust, perhaps that idea can be trnaslated over to the world of RL or just ML in general.

Tools such as Mutual Information, as opposed to measures of correlation or Pearsons coefficient of determination as well as regime analysis and the previously explored power law fitting of data may help provide insight into how one can watch and add value in fat tailed or multi modal situations, by effectively offering to the operaator how to tailor exposure/show what the various regimes may look like.

If there's heuristics such as intermediate shapes of a distribution for a sum i.e. rates of convergance to figure out what kind of distribution a variable is being approximately drawn from assuming the soft conditions of independance and identical originating distribution. Then it could be the case that RL or just NN could also devolp better heuristics, or different heuristics and tell us which distributions we might be looking at. Taken with the distributions which we derrive it might be from the regular heuristics and rules Taleb reveals gives us potentially greater insight to what we can say it might not be, and what the worst case distribution we need to protect against. 

This could possibly be done by making the reward, the percentage likelihood that a stream of values observed came from a certain distribution with certain parameters and make the penalty the max confidence the machine had in a distribution being a candidate prior to it being disqualified (<0.1% confidence of it being a candidate). Possibly could use just the penalty but I think the rough idea makes sense.

Tests such as using the EVT heurstic of sigma(u) which supposes that the difference in probability of a maximum under an unknown distribution should as that maximum goes out further and further towards the finite or infinite right endpoint of the unknown distribution have it's conditional distribution beyond a given value be no different than if the distriution were some Generalised Pareto Distribution. (Though I'm not quite sure how this ties into the theorem about the GEVD, and the idea that the MDA for any power law distribution is the Frechet distribution).

Most of the time for a security or whatever is being analysed it is sufficient to realise what class a given variable likely belongs to, and then proceeding to explore that class for what distribution it may be, and what parameters it may have like tail exponent, location, scale, shape, etc.

Potentially as opposed to using sigmoid, or leaky ReLUs, a use of weighted calls, with different times to expiration to create curvuture may be useful, though it would certainly need to be tested. 

Additionaly by performing a "battery" of tests, we can find not only if a tail is Paretian in distribution but also potential assymetrys and dependences by noting changes one unit removed in time (before and after), and then comparing the number of maxs to a Harmonic Number of how many maxs would be the case given random shuffled data. 

If you have data generated from a right skewed distribution with infinite variance and a bounded left side then you will likely endup with partial sums in their pre asymptotic phase producing lower "mean" values than the true mean. This is the case as the mode is to the left of the mean resulting in a lower value such as discussions of Gini and percentiles. It also creates the illusion of things such as increasing inequality due simply to the fact that a better approximation is occuring due to a higeher n value bring the mean closer to the true value. However this can be somewhat fixed by using a tail exponenet approximation via the log-log linear regression and using the lowest tail exponent or generating a stochastic tail exponenet, which can both serve the purpose of allowing for extrapolation and generating a psuedo distribution which can tell you how far the mode is from the mean thus allowing for a band aid in the form of a semi corrected parametric estimate.

If you have a set of data which appears to be of an undefined mean i.e of alpha < 1 but you know there's a maximum like for example, a companys sale are topped out by GDP, or the loss in value of an asset is bounded by it's current full capitalisation, then you can use a method by which you take the tail as is, and then find some coditional mean in the tail beyond some threshold via some fancy math, and from that work out the properties of the original distribution that could help in deciding what could happen on average, with war for example Taleb calculates it as casualties are bounded by the human population, and beyond some threshold like 50,000 thousand casualties, the conditonal mean is about 30,000,000 which is roughly 3x as much as is the case with the simple arithementic mean, therefore from a strict statistical point of view the world is possibly more dangerous than we think and violence may not be on the decline we might just be in a good period.

Given that we try to estimate an amount of uncertainity about the average of a distribution via the standard deviation / variance, it would only make sense to estimate some level of uncertainity about that uncertainity, and keeping constant with that train of thought and assuming we have no level of uncertainity about which we are certain then we can go infinitely deep, leading to an explosion of the second moment which creates very thick tails. So unless there is a decreasing level of uncertainity as we go out, or some point at which we are sure that's the amount of uncertainity we explode out effectively to power laws. Or as Taleb puts it, "Fat Tails are the Product of Recusrsive Epistemic Uncertainty".

Also in the same way that OTM options are convex to Std. Dev. as in as the Standard Deviation rises by 1 unit the value of the OTM option rises quicker than 1 unit and that rate accelerates the more Std. Dev. increases. They are also convex to uncertainity about the Standard Deviation, this is because as uncertainity about it rises it could be higher and so prices of OTM options rise. And in the same fashion the expectation or mean of a power law distributed variable is convex to the tail exponent and the uncertainity about the tail exponent. This convexity to uncertainity for some reason though appears to make the true mean assymetrically higher than lower than the in sample mean, leading to a further downward biased estimate of the expectation, but why and to what degree I'm entirely uncertain.

Given that we take p-values to be generated of a power law meta distribution, the probability of atleast one p-value in a series of expiriments being less than 0.05 is almost 100% even if the truth of the matter is weak or even no evidence against the Null Hypothesis, just due to the stochastic nature of an expiriment. The fixes suggested are looking for p-value < 0.005 or my preffered solution is to report the full distribution of p-values expirienced as the expirirment was done over and over as opposed to just reporting the minimum/statistically significant point p-value, so that it can be understood and fitting a distribution to it we can find the median p-value and MAD etc. to get a better idea of what's going on. Obviously stochasticity of the tail exponent introduuces further complications, but perhaps not a bad first step.

CLT states that your partial sum as Sn = A large enough number, the distribution will come out to being Gaussian. The Law of Large Numbers then says as the CLT kicks in your mean will end up concentrating and concentrating until you get a Dirac stick for Sn = A very large number i.e. low to no variance as n gets vry big. The problem is for power laws it takes a very very large n for those 2 things to kick in, assuming there even is a mean or a finite variance.

There is possibly much to be gained even under fat tails, to making bets with the Kelly criterion, thereby avoiding any chance of risk, as the geometric mean is maximised and greater than 0, and also to the idea of the St. Petersburg resolution of buying insurance on risky instruments, to boost the geometric mean, at the cozt of the arithemetic one which was also emphasised by Spitznagel, and also the notion of volatility harvesting as a way to allow for the use of arithmetic average to be in your favor, given taxes and commisions can be minimised in such a way to allow it to be viable. This allows for better thinking on a portfolio level, under any regime, pbviously however correlation of risks still need to be considered and diversified to whatever extent they can be or countered via insurance, etc. It should be added however that a fractional Kelly bet is better as there are things such as uncertainties due to epistimic boundries, so even with a pretty suree thing, one should bet less than Kelly optimal, even after the kelly optimal has been reduced to be conservative, this allows for possibly over conservatism in some sense but also defenitley a lack of overbetting, and thus does not allow for the ruin. 

Additionally ideas of information such as entropy, and the notion of a private wire giving truer odds than is the case being priced on the tote board or in the market alllows for better thinking about the fundamental analysis of a situation, and how best to bet on any given situation, as well as allowing for a more disinterested more distant approach to equity selection.

I am now convinced of the idea that through thurough witiling down of possibilities is the best way to approach almost any problem. With a disconfirmatory cynical perspective, than with a onfirmatory positivisst perspective.

## With Repect to Work and Learning

It is not unreasonable to approach any obstacle of life or work as things to be tackled by a process of refined reasearch, with hypothesis, observations, analysis, and what that could possibly tell you about the domain with which you're interacting.

I find more and more that where things, fail, break, and don't break is substantially more important to understand then where they do. I suppose this is encapsulated in the 2 phrases, "It ain't what you know, it's what you know for sure but just aint so." and "I want to know where I'll die so I'll never go there". I think possibly just following those 2 ideas and constantly looking for where failure is likely and avoiding them leaves just the success to take care of itself.

With respect to incentives, if you can learn the incentives of others and choose to be incentivised by something else you can focus on the fundamentals while giving enough of the thing the other is incentivised by to gain what you desire as secondary goals etc. (For ex. Some look for the Githib commits, so a few a day, or even 1 a day is enough to give the recruiter the feeling they can sell me, etc. and I can at the sam time mark myself by how cool my projects are etc. but in doing enough of the first I can get a good job, cool interviews etc.)

It appears to learn anything in the real world you have to really really struggle, and deal with these very treacherous learning environments. Some of it comes easily in plain english, but there come cracks and nuances which are too large or difficult to capture with verbalistic notions. On the other hand, there are these deeply thought out, rich texts which require such a great degree of contextual knowledge and fundamental knowledge of assumptions that it not only overwhelms but can very easily trick you if you are not comfortable with trying to see where a theorem, idea, or model fails, especially so if you don't take care to appropriately hedge against error occuring.

Perhaps this doesn't need an explicit statement but I've noticed that the ability to learn an idea deeply rests both on giving oneself time to digest an idea, while attacking it from multiple angles but it also resides in the neccessary mandate of actively engaging with the idea everytime one engages with it.

As I get deeper into dedicating myself to actually studying and pursuing intrests, I find I am bound by 2 things. 

    1. Intrest in other things spikes up and I am unable to get to it, until I finish what currentlyholds my intrest. This is primarily because if I allow myself to get interested in a lot of things I'll start a lot of things and never finish a lot of things, which I would find more unsatisfying than simply having done fewer things.

    2. My focus can be very variable depending on the day, and I often find myself in this state where I want to learn yet my brain has no capacity for the focus I need to put up and the lack of stimulation from the thing. The latter is begining to wane somewhat as I become more and more comfortable with things like reading and watching lectures. Yet the first acts as a very hard obstacle although my genuine disintrest in more uninteresting things is acting to help me feel more comfortable. I find it's defenitley a tradeoff. But one which is proggressively more favorable.

I've found additionally, there's a sweet spot to be struck between understanding a concept in full depth and just accepting that it will make sense as the greater picture comes to be, and if it still bothers you once you've gone deep into the dexcursion you can always come back to it.

Also I found additionally, the smallest possible step is that which is repeatable every day.

In addition to approaching learning from this more relaxed, yet consistently struggling way and by focusing on taking time to digest and break down ideas in your head, it is not ideas that you learn, but rather tools that you can then use to cut a problem down, analyse the subproblems, solve the subproblems via a method of rigirous disconfirmation, understand their consequences, and put it back together in a comprehensible and usable way for yourself and others.

Deep unthinking relaxation is also profoundly important to not just learning, but to the congealing of oneself with their humanity.

It comes to be that almost all political situations boil down to goals, available paths/branching paths, incentives, and leveraging actions to generate the largest optionality, this comes as the natural other hand to single minded obsession. 

Problem solving effectively boils down to understanding the concepts, principles, and techniques you can apply to a situation, figuring out the structure of the system in which the problem is encased and exploiting said structure to have it be shaped more amenably. It also comes to how to come up with ideas when you run out, i.e. perturbing the structure, which comes with gathering more tools and where to look for solutions.

Failure to take things from theory to reality is likely a function of mismatches between the context of theory and the medium to which you are trying to translate that theory, i.e. rhymes to paper, books to excel forecasts, or algorithims to code.

Truly resting is a critical skill, yet one entirely ununderstood by myself, and likely everyone else who works more than they think.

It is certainly curious that rest requires a slowness and abatement which the ideal of consistency cannot permit. I wonder if those things thus become related as in those who can fall back and take a moment can then speed up and get ahead, akin to the ideas in the Art of War.

The balance in order to achieve competency and the volatility that is correlated with progress have to be finely balanced lest one become complacent or unstable.

I also seem to get very wrapped up in my own thoughts and context to the point where it becomes indiscernable from the have to's of my day like breathing and eating. However unlearning that, as well as how to commit tentatively such that I can reallign as my goals change will be a massive boon.

With respect to making things you just go layer by layer like a painter, and as you layer better and better you get closer and closer to making the beautiful art you wanted to make even if it's radically different from the intial vision. Also remember you go from:

A -------> A
↑          |
|          |
|          ↓
S -------> S

Where S stands for situation and A for abstraction

Learning appears best done in the order vocab, grammar, and historical developments with repect to whatever field is being dealt with. Vocab being the terms, grammar being how those terms are then used in various contexts and what they do to the end result of a statement and then historical context being how others in that field developed the vocabulary used, contending ideas thrown out, etc.

There seems to me 3 points to learning what I'll call, Engineering, Science, and Construction/Bridging. Engineering is devolped by creating things via exploitations of the underlying structure, why those exploits may exist, or what fundamentally the problem being solved is is kind of unimportant only a strong comprehension of the technology is required, like doing LEGOs. Science doesn't care about solving any problem it's particular intrest is what is the structure and what/why are the exploits, it seeks a deep comprehension such that the object or phenomena of intrest can most aptly be mapped and modeled. Finally, neither the technology nor the structure is of any intrest to the Builder, only the problem, how to decompose the problem, and how to build the solution by putting things together in any permutation which works.

This can be seen in atleast the 2 fields of CS and Investing, with respect to the Engineer one would find intrest in the specific structure and working of Docker, AWS, puts, futures, etc. Why they work that way is unimportant only that they do and they do so smoothly. The Scientist in each of these domains is then rather intrested, what makes a good model of the problem to be solved, what are the algorithims and heuristics which could effectively be leveraged if someone wanted to solve some situation, for ex. in CS it's like what way should a factory be represented on a machine, what mathematical attributes describe it, and in finance it's like how should a companys value be discerned, what tricks can we use to convert various features such as EBITDA or Current Assets - Total Debt to tangible understandings of the value/risk profile of an oppurtunity. Finally the Constructor doesn't care about what is the exact way Docker or AWS or a put or a future works or about how do we model this, how do we evaluate this, they have a task, and they create a solution with the tools available to them wether it's ugly or not from a smoothness of running or theoretical perspective is entirely not their concern.

There is at every level dimes on the ground, easy money to be picked up, be it the proof of naive set construction to be contradictory, the consequences of a universal speed limit, the idea of intrinsic asset values, wheels on suitcases, or the very idea that all things don't happen the same way under every repition of the exact same intial conditions. This is the case because at any given time there is only a finite amount of research going into any one issue or field, and for all sorts of reasons people overlook things. So just as with equities looking where no one is looking, for reasons of difficulty, boredom, etc., having a strong grasp of the situation and the language by which to decode it to it's fundamental attributes, the willingness to do the legwork to find it, and also just being keenly observant and following those observations down to where they lead and make coherent sense even if it's contradictory to your knowledge will yield great fruit.

I have a tendancy, and I think more broadly people in general have a tendancy to get too into picking up what appear to be gold nuggets 
in the form of the latest or emerging thing. While certainly, techniques, technology, etc. are fascinating to learn about for their own sake, often as far as the real world is concerned only about 1-5% of the things ever thought or devolped will be left standing after 5-20 years, so it becomes more fruitful to both gleam all the nuggets still standing in every field, and to work backwards from problem to solution rather than solution to problem. The additionally difficult thing is that we can never particularily know what will wash out, and what will stay and become one of the supreme workhorses of that fields toolkit.

No one really knows what they're doing, it's really not that deep for most people.

To understand something from first principles has to me meant, that you understand it within the framework of vocab, grammar, history. That is to say that you learn to read the idea embedded within whatever mental representation it's encased, even if you don't understand it, then once you have an ability to read the primitiaves (think mathematics or accounting), understanding the primitiaves in such a way that you understand when a primitive shows up in a certain location what that might mean for the deeper meaning of the larger structure being put together (think like a coefficient next to a vector in quantum mechanics signaling the superposition of the state or cash and liquid assets decreasing with a roughly equivialnt increase in long term assets and no change in liabilities indicating increased investment etc.), then finally understanding the context in which the terms and equations you are seeing proved to work were devolped, what problems they addressed, and how they've been evolved through time in some sense. At such a point you will be able to read, understand, and have the intuition behind the model which you are seeing making application much simpler, though this is certainly a longer method than just learning when you can and can't use it, it likely has many more benefits and allows for a greater ability to interpert the consequences of and make use of the model.

I think that on the problem to problem side of things, it makes sense to just use a straight up engineered product if ti adaquately suits your needs for whatever you're trying to do. 

However if such a premade solution does not exist in your circle of knowledge, and you have to engineer your own solution then after abstracting to the mathematical space and derriving what your input is, and your output should look like, you can find a verbalistic description of an idea to put forth (oftentimes a pattern), which you can then engineer to solve the issue of mapping, for example under a GAN framework, you could generate two RL agents playing a minimax game of sorts, or you could simmulate it as an art forger and inspector based on CNNs etc.

If such a system is adopted and you have a verbalistic and engineered mathematical simumulation by which you have arrived at a valuable mapping between input and solution states, then neccessarily there may be consequences such as under the GAN framework, various trajectories in the latent(input) space, will correspond to the mapping function generating different types of images etc. which are then their own interesting scientific pursuits.

I think with respect to any work begining first by understanding deeply the environment and space of the problem though time consuming can lead to much clearer visions of what is to be done should a certain goal be fullfilled.

You get to choose your own customers, and at the start you only really have one customer to choose from, you. After that though you can start listening to which customers you like and find their critiques to be reasonable and something which doesn't somehow make your expirience with the thing worse.

Once you have your 2 things (1 personal and 1 field application type thing) which you want to be the greatest, chase that and in doing so you'll naturally find the application of techniques you're interested in, in things which lie outside the immedeate "field" of the thing in which you want to be the greatest. But reading broadly for exposure to ideas just to see how things work out is still a good idea but only in time you're not spending getting better at the thing you want to be the greatest in.

Going to the level of research papers, or annual reports is a secondary step after the vocab, grammar, and history are relatively well understood, to approximately the graduate level. After that's done the deeper papers on which to form your opinion are something to be accessed to either 1. Learn more for breadth/depth purposes or 2. Solve a specific problem you are facing, other than that though there's no point of putting yourself through it.

Work is as much self selection as anything in the world, because you have to act in a fashion agreeable to the system which provides you the nutrients, sufficient you are able to enjoy the labor or able to garner the rewards via your labor and able to express yourself as you are, you are able to and good to keep surviving in that place and being yourself you are in fact living. In the case that is not true, i.e. For one reason or another the system about you rejects you or fails around you, it is simply your perogative to find another in which you can garner rewards by your labor and be yourself.

With respect to the specifics of machine learning work, or more accurately the devolpment of solutions for money, there's a simple 5 step chain: 1. Frame the business problem/method of value add in the simplest way, 2. Understand the structure of the problem by formalising it from 1. to a computable/mathematical (domain reasonable) problem, 3. Build/use the simplest/most common solution to the formalism of 2., 4. See how/why the method in 3. fails to deliver exactly what you need and iterate, 5. Adapt/refine the solution in a reasonable fashion from 4. to what the customer wants/has ease with, in terms of use.

There's really no book or video or podcast or anything else which acts as a substitute to raw legwork, just doing legwork over and over while being curious, process focused, intelligently risk bearing, etc. and just constantly going for whatever it may be that you want is all you need to do and all that really is worth anything. The reason for this is that all those other things are like magic, as in they are magical things they are enjoyable and entertaining and stimmulating, but the truth is the process of creation is actually rarely such a thing, it is the constant balancing of immense stress and pressure in what feels like a futile attempt to make somethimg you know has no significance against sanity. Such a process is unbearable if you desire magic, therefore though magic is good and worth taking time to expirience and live in the mind, there is only one perscription should you desire to actually create and not just learn, legwork and robust implementation.

It appears additionally to me from what I've observed that there's no value to considering the time components of things in non linear activities such as creation or generation or research only showing up chopping the wood, carrying the water, and planting the seeds, and a belief in oneself along with a belief in a kind universe which atleast sufficiently vindicates someone who is trying their best and that things will thus get to be when they get to be, as in don't worry about time just do the best you can in a given moment and know that your best is your best and it will generally be sufficient and when it's not you can learn.

Finally it appears to me that while productivity drops off with time, in a state of flow less mentally taxing tasks become almost natural given mindful application of oneself, and also there appears to be no way to avoid obstacles. Only to confront them and truly wrestle with them even though you have not the briefest clue of what it is you might be doing.

You have to actually do the work not just have ideas.

Take everything to be true and unnecessary of proof, on the first pass, after that on the second pass you can ask what the implications of those truths are, and by the third you can begin to reason as why they may not in fact be true.

Making mystical hypothesis is the bread and butter of any scientific research, it is the essence of progress, but one has to remember not to get caught up on it, rather to massage the theory until you get a prediction from it and then to adaquetly test the prediction by means of naive falsfication using an expectations based Bayesian Solution Tree.

Set up a tangible goal, believe in yourself, God/luck, and put head to grindstone and just commit work every day to sharpen the ask, so that once the day comes to do things you can be undeniable. Simply iterate, improve, and keep seeking the truth of the system.

One devolps physical theories i.e. theories of the input output reactions in the world via trial and error, and those theories are then affirmed, discredited, or revised based on the ability to generalise them from observation, to statements of statistical, logical, and =1 mathematical accuracy which reflect the underlying generator better than a single set of expiriments. This is true for machine learning, and computing, economics and the stock market, and probably also the universe as a whole. Throw things you understand at something you don't see how it reacts and responds, and then find a way to exploit the pattern of input/output behaviour via intuition that you scratch down in symbols to represent what you need.

## With Repect to Teaching and Sharing

When you have people of all different cultureal contexts, domain knowledge, incentives, and levels of resistance to different proposed ideas, one has to fundamentally come at the practice of teaching and sharing as not correct or incorrect but rather perhaps a fundamental set of tenants and beliefs, and the exact scope of the conversation.

Empathy is also extremely important. As the state and beliefs of an individual will really make a difference to how the conversation should be had.

No matter how many successes you have, you will always have failures, and they will almost certainly loom larger than any win can make up for however, the key to it is to keep going keep moving, and keep trying. Because as long as you are not defeated, you are not defined by your losses.

Deep traumas or even misalligned feelings relating to things like work, and research etc. are one of the key hurdles to enjoying life to the fullest, because meaningful work is one of those things that you can not have for a long time, but once you get your life becomes much more satisfying and well paced.

You do become at some point the person someone looks to guidance be ready for the responsibility by making as many mistakes as possible without doing anything obviously dumb.

No matter how much or how little you get done, life is well lived, given that you've tried your very best to live it.

Additionally that which you do more days then not, becomes that at which you are better than most, and becomes what you are more than most.

A perfect lecture would strike the balance between concise, conversational, and information dense. It would likely be the result of shaping the audiences environment to ask certain questions, answered to compliment and pass information into their mind such that they can more easily synthesise the statements being told to them.

Definitions and intros are the scariest bit, however once it's understood just as vocab i.e. the tools of consructing everything else, is the hardest bit especially in isolation. But once you learn the vocab relatively well, the grammar and so on becomes exceptionally easier because you understand the things you're saying, hence you can tailor it to what you want to say.

Never speak on anything you have not entirely rederrived for yourself.

Switch between any 2 reasonable hypothesis (any hypothesis which to a reasonable degree fits the facts of which you're aware) should one be more probably useful.

## With Respect to Finance

Perhaps because the question of how much should a lender charge a borrower in isolation is an almost to impossible question to answe, we swapped it out instead for the question of what rate can you charge to maximize profit for the firm? 

I think there's 4 schools of Investing all of which are images of some deeply unified theory. Value, which states small and difficult to love is the place to be, Growth, which says things with good returns on capital and growing therefore generating more earnings in the future and reasonably priced is the place to be. Additionally, there are the schools of Chaos, which states simply do whatever you want for any reason but make sure you're protected and insured, as long as you're in the game keep trying and doing things technical or fundamental it doesn't matter, and finally Mathematical, which says you can by sufficiently modelling things predict on an aggregate basis what will happen as with classical mechanics and therefore drive some returns on a large amount of small advantage trade.

## With Repect to Greatness

Being considered great is a function of the historical process and the human memory. Both of which are outside the realm of your control, geniuses are forgotten and fools are immortalised. One can only foxus on maximising their chances at greatness by truly being great, which means one can only maximize their chances by doing the work and pushing oneself without breaking.

It is highly correlated with doing things unconventionally, or in a way that is public but remote, surprise I suppose is the largest correlative factor.

Greatness is not a product or contribution but rather who we are in every moment we are. What makes heroes and people great are those who never quit trying to be good, unencumbered by money or any incentive other than to be as good as possible. But all those ideals feel fake and 2 dimensional, to be real and great is something more profound and unknown then that. It is in accepting failure, showing up, being reliable, looking out for people when you can, and trying to spend the brief moments we have on this precious jewel expierencing life doing something mildly interesting and having fun. There is no purpose to greatness other than to serve as an other objective we can seek to optomize, but there is nothing to optimize only to expirience and you should just pick expiriences which lead to more and more desirable states, till you have one beyond which you do not need, and resist the want for more.

Not everyone starts gifted and privelleged from birth, we just start out where we do on our own broken path in life, and if we get lucky so we get a fighting chance, take risks well, and leverage the situations adaquetly to the extent we are able i.e. give it our best shot, then we may end up at a place we cannot even begin to imagine but can only hope that we are still kind and able to put love back into the world.

Fundamentally the whole idea behind incentives and the reasoning for any action can be broken down into a belief of how one can go about maximising time spent being free (taking any action and getting the result of the best action on a 100% deterministic basis) and being bound (each action has it's own value and thus some are prefferable to others under the constraints of the situation). The greater the split of ones to time to the former the freer they are the more it is dedicated to the latter the more dutiful and advanced one becomes. Ideally there would be sufficient of the latter such that one can spend a greater and greater portion of their time doing the former. Thus in effect maximising it, and this question then effectively drives all our actions, with our subconcious correlations filling in what it is we need to accquire/own/have in order to get to such a state.

One has to understand where if their current potential is maintained how great they can be, and one must be staisfied in both knowing where the boundary of that is, and wherein they can grow, and when it is sufficient for life enough to be like a dream. As well it helps in being able to recognise what kind and how great someone else is, it can allow for you to surround yourself with greats, younger, older, and of the same cohort as you and allows you to know when to listen to someone else and when not to. I t allows for the creation of the use of friends and resources while limiting or prohibiting the abuse of said friends and resources.

Science is much like all of expirience itself a very personal pursuit. Used to address the issues and difficulties, and make real the dreams of an individual, therefore in addition to doing it yourself, supporting those most marginalised to do their science their way is the best things we can do to promote the dream. To make as many scientists as possible, not just data crunchers but creatives and artistic types to become scientifically savvy.

I think the commonality which I find to be most intense among all the people I consider truly great is the dedication to the fundamentals to ignore all the noise and distraction of approval, and results. To focus single mindedly and obsessively on the fundamentals of the entity to which you are applying your process of deconstruction and solution to and the process of deconstruction and solution itself.

The second is having great people around themselves, which I suppose now in the internet age is a lot easier, and there's a lot more great people with which one can surround themselves.

The third is working on cool and great things, which also is easier to find, and more to do now in the age of instant communication, and centeralised knowledge stores.

The attempt at a problem without the desired results is not failure, rather it is a step in the corrosion of a mountain.

## With respect to Science

All science effectively boils down to discovery and development, see a problem which intrests you for some personal reason, popularity, money, artistic expression, equalisation, etc. etc. and then discover abstractions which allow for an acceptable amount of lossiness in the abstracted state, then devolp techniques for that state which leverage the paarticularities of the abstraction and then recovert in as lossless a way as possible and then if desired refine and do it over and over again until satisfied.

Science in my opinion is not even neccessarily the understanding of why something works, it is simply the finding of repetable behaviour, if you can find behaviour which determinstically, probablistically, or randomly repeats in exactly the same way. That knowledge can then be documented, and digested to refine how it's leveraged.

Science is also the great bastardizer of our myths showing us what is and most often what isn't the case if we're just willing to see through repeated trial and expiriment but in doing so also shows us the true depth and richness of life.

People used to have math duels and math partially to one up each other and solve strange occurances of logical consequences and occassionally model with them. Weird world, weirder times without much incentive to keep secrets of mathematics.

The great marvel of the concious mind appears to me above everything the ability to store patterns and imagine or dream. In these imaginations and dreams we take the problems with which we are faced and interpert based on our understanding of the charachteristics of the problem losing whatever we feel may be unimportant chareteristics, we then leverage the tricks applicable to the abstraction and once solved and proved to our standards we then act to implement the solution we found acceptable and over time having done that enough and devolping more abstractions, more tricks, and finding more charachterisitics of all things, we now find ourself in the present wonderful state of affairs with computers, air conditioning, and all the other modern marvels which allow us to find whatever thing will make us presently happy.

The present state of discovery it must always be remembered is a function of the beliefs, ideologies, and incentives of the discoveres for ex. the Intuitionists blocking Cantor, or Russel disproving that set theory is logically coherent, and the reaction as such are all as much acts of human ingenuity, science, etc. as they are plays of economics, psychology, politics, and clashing philosophies.

Secondly to the extent that any model is accepted as a representation, the deep underlying axioms taken to be must be proved rigrously to not seriously violate the problem attempting to be solved, do so and you may end up with a solution like Efficient Makrkets or the Black-Scholes to the problem of market dynamics and pricing. But also altering the lens by which we view a problem so that we construct out a mathematically meaningful model/representation in a different space where it does not violate the assumptions/axioms of that space either can lead to intersting new ideas, for ex. taking Machine Learning out of Euclidiean Space and reconstructing the loss landscape under a spherical geometry or hyperbolic geometry and seeing what can be done as to finding a minimum loss, same with removing requirements such as normality in underlyting distribution etc. can free us to see things differently. That's what the chemists who used Knot Theory as a way to model their molecules did, and it led to a very new way of thinking about certain things.

One can only exploit the properties of a problem with their tools given that the problem is well formalised into a structure with a clear goal state that can be considered the solved state, and the tools applied can only allow speed ups and alternate equivlances of legally allowed moves in said formalised structure. If you simply violate the structure, or ignore the requirement for structure in the first place you deteriorate the quality of your solution.

Client -- Produces -> Problem -- Interperted -> Problem Statement -- Conceptualised -> P-Space -- Converted -> M-Space -- Reasearched -> Prototype -- Tested -> Solution -- Productionised -> Product -- CI/CD'd -> Application

A model has no value for a models sake. Rather we define a model so that we can play with it and get consequences. This idea of consequences is the deep value of a model, take for instance the plum pudding Model, it implied a certain result given a certain action, for instance is a bomb goes off somewhere it doesn't affect the model in a strict sense under the assumption the bomb doesn't affect the atom, which it is supposed to represent. However you can perform an operation such as firing an electron at it which would reult in the electron getting stuck in the plum pudding, now this might not be a great model of an atom, but it is a good representation of something else where you can shoot things at it and it will hold onto the thing shot at it in a sticky viscous manner. Models then act as like analogies or metaphors that we use to quickly ascertain what actions are permissable and what consequences various actions should have (for ex. electrons should not pass through if shot at the atom). These actions are not neccessarily a given, but lie within a finitely permissiable subset of the larger actions set (for example you can't copulate with the shrodinger equation, however hard one might try, in the sense of physical penetration, but you can multiply the shrodinger equation by any other term, which should have a known consequence which is that most of the time you get nonsense, or flip all the symbols upside down, which should not change the equation per se. However if flipping it upside down for some reason gives you Einsteins general relativity then there is something now to investigate why that might be.) So models effectively act as a map or a function in a very loose sense, between actions one can take and consequences or results, therefore if you pick/generate a good model which is well established/explored or has for some reason a well defined set of consequences to where an action will map, then you quickly can find actions which get you from your current state to your desired state. It should be clarified though that no part of this involves altering the model, only in altering the state of a specific instance of it.

Don't seek to solve, seek to understand. I suppose that's truly the difference between the corporate framework of science, and science  under a more purist framework, that the former framework seeks to understand only to the extent of exploitable purpose, however the latter while also sparked by trouble, curiousity, or simple subversion of expectation is not pursued to find the answer which somply results in the current problem being fixed. Rather it seeks to understand why a phenomena is so, and that's it, just put it aside after that basically as a notion and tool, which can then be used to model a solution and then maybe it works maybe it doesn't and you just repeat, until you run out of curosity, trouble, or the expectation is no longer subverted.

You can redefine anything for ex. saying quantum physical particles have some feature they do or don't have and then see what the consequences may be, the same is true for the mathematical structure, for ex. you could redfine division as 
*/ := (floor(x/2),ceil(x/2)), this then leads to certain consequences and a new math, and so it goes for everything else in the world, defenition and consequence. 

## With respect to Data Science

Manifold learning, is the learning of the shape of the manifold that matches the shape of the data in it's higher dimensional euclidean affine plane/space. If the eath is well understood it could be very useful in getting the data in a much more useful form. However my intial search was on trying to learn the shape of the loss landscape because like y is a function of the theta, i.e. loss is a function of the weights and whatever else is going in the machine learning model to output the numbers it outputs. I think that scattering theory idea might be interesting but the loss landscape changes for every set of inputs from the latent/input space. For ex. if the values [1, 2, 3] have to be mapped to [2, 4, 6] and the value [2, 3, 5] have to be mapped to [8, 12, 20], the value of that loss function will be lowest at c = 2 for the first one and c = 4 for the second one. There may be a shared structure of that loss function among those various latent space inputs with simply different locations for the global minima, there also may not be shared simmilarity among the various loss functions from the various latent/input space inputs and the output vector in output space that we desire. But there may very well be a way to prod the loss function to find its shape better given a certain mathematical function, once we have this we may have more intelligent ideas of what to do or how to do it. Because if you change the function by adding more layers etc. you change the shape of the loss function, I believe. 

Even if you can't say what the function is, you need to be able to say why a point in your input space should map to a point in an output space if you yourself don't have the clarity there's no reason a machine would.

Imaginative Philisophical insights will lead to the next edge of machine learning/computing/math/physics etc.

A method of humans being able to turn their knowledge of why an output is wrong into changes in the weights of the neural architecture is proably a good step forward, along with regular debugging and visulaisation techniques which turn the network into something tuneable. Also a good set of inputs are ones which map well to the given outputs by logic or something and a good set of outputs are ones which are easily mapped to, making the outputs something easy to learn means the NN is more likely to get it right and then passing that off to the next model or part of the algorithim is probably the way to go.