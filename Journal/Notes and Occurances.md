# Thoughts and Occurances

## With Respect to the Consequences of Fat Tails in Statistical Consequences

It may be that the very infrastructure on which we model may be wrong, and that far too many normality and modelability assumptions are made, and this may present a crack that has yet to be discovered, and requires only such a tail event to occur for it to blow up on us. Though it is always possible that rather everything is fine.

If not however it may reveal to what more things are certain to be beyond our grasp at the moment, and could potentially focus efforts or just make use of data science, where such use is good.  

In addition, it may very well present oppurtunities for new investing practices, such as effectively short selling hype, the way value investors buy statistcally profitable bundles of cheap securities. Perhaps they may even reveal things about the others such as growth about value and value about growth, fragility about value and value about fragility. As well it may present oppurtunities to reconsider a host of other decisions entirely built on on flawed foundations.

It appears most RL and other neural network techniques also tend to be dependant on assumptions of normality and tending toward normality sufficient a large sample size.

If this is the case then perhaps an RL model can be devolped such that it can more quickly determine what kind of function, or tail coefficient a distribution should be generated with and thus behave accordingly.

Watching for large deviations within a collection of data i.e. outliers of some consistent but low frequency is likely the best way to find if the domain is fat tailed, and so dangerous for things like an RL model to learn/predict on. 

However the solution cannot be to just simply do nothing, even for us as people we have to learn to take appropriately hedged bets meaning thurough understandings of risk, which itself may be something that might be difficult to analyse from a statistical perspective i.e with use of past data, though it may not be. This may be one of the interesting ideas to investigate.

A point about Standard Deviation and beyond, for any 2 vectors assuming their L1/mean is the same if the dispersion is wider for one than another, say [1,1] and [2/3,4/3] the l2/STD is going to be larger, and the difference will increase explosively the larger an l norm you take. 

Hence Taleb advises using the MAD for both stability,and communicability of an idea of average deviation, though even still you can't make much of a prediction with MAD, it's just more useful to describe what it ddescribes. So I don't know how we would solve the quandry of what action/exposure to take for an RL prgram, except for that it must be a hedged one in potentially fat tailed situations. 

Infinite variance models based on thing like Cauchy Stable or 2 sided Pareto or Student T with a small tail exponenet can all simmulate fat tailed conditions, although they may all have different shapes, the tail exponent chosen for a Student T, does appear to be pretty robust, so it can occasionaly be useful to see grey swans assuming the data fit reasonably well. This means we can find a reasonable price for the insurance of a situation, as well as how to intelligently take exposure so we don't go bust, perhaps that idea can be trnaslated over to the world of RL or just ML in general.

Tools such as Mutual Information, as opposed to measures of correlation or Pearsons coefficient of determination as well as regime analysis and the previously explored power law fitting of data may help provide insight into how one can watch and add value in fat tailed or multi modal situations, by effectively offering to the operaator how to tailor exposure/show what the various regimes may look like.

If there's heuristics such as intermediate shapes of a distribution for a sum i.e. rates of convergance to figure out what kind of distribution a variable is being approximately drawn from assuming the soft conditions of independance and identical originating distribution. Then it could be the case that RL or just NN could also devolp better heuristics, or different heuristics and tell us which distributions we might be looking at. Taken with the distributions which we derrive it might be from the regular heuristics and rules Taleb reveals gives us potentially greater insight to what we can say it might not be, and what the worst case distribution we need to protect against. 

This could possibly be done by making the reward, the percentage likelihood that a stream of values observed came from a certain distribution with certain parameters and make the penalty the max confidence the machine had in a distribution being a candidate prior to it being disqualified (<0.1% confidence of it being a candidate). Possibly could use just the penalty but I think the rough idea makes sense.

Tests such as using the EVT heurstic of sigma(u) which supposes that the difference in probability of a maximum under an unknown distribution should as that maximum goes out further and further towards the finite or infinite right endpoint of the unknown distribution have it's conditional distribution beyond a given value be no different than if the distriution were some Generalised Pareto Distribution. (Though I'm not quite sure how this ties into the theorem about the GEVD, and the idea that the MDA for any power law distribution is the Frechet distribution).

Most of the time for a security or whatever is being analysed it is sufficient to realise what class a given variable likely belongs to, and then proceeding to explore that class for what distribution it may be, and what parameters it may have like tail exponent, location, scale, shape, etc.

Potentially as opposed to using sigmoid, or leaky ReLUs, a use of weighted calls, with different times to expiration to create curvuture may be useful, though it would certainly need to be tested. 

Additionaly by performing a "battery" of tests, we can find not only if a tail is Paretian in distribution but also potential assymetrys and dependences by noting changes one unit removed in time (before and after), and then comparing the number of maxs to a Harmonic Number of how many maxs would be the case given random shuffled data. 

If you have data generated from a right skewed distribution with infinite variance and a bounded left side then you will likely endup with partial sums in their pre asymptotic phase producing lower "mean" values than the true mean. This is the case as the mode is to the left of the mean resulting in a lower value such as discussions of Gini and percentiles. It also creates the illusion of things such as increasing inequality due simply to the fact that a better approximation is occuring due to a higeher n value bring the mean closer to the true value. However this can be somewhat fixed by using a tail exponenet approximation via the log-log linear regression and using the lowest tail exponent or generating a stochastic tail exponenet, which can both serve the purpose of allowing for extrapolation and generating a psuedo distribution which can tell you how far the mode is from the mean thus allowing for a band aid in the form of a semi corrected parametric estimate.

If you have a set of data which appears to be of an undefined mean i.e of alpha < 1 but you know there's a maximum like for example, a companys sale are topped out by GDP, or the loss in value of an asset is bounded by it's current full capitalisation, then you can use a method by which you take the tail as is, and then find some coditional mean in the tail beyond some threshold via some fancy math, and from that work out the properties of the original distribution that could help in deciding what could happen on average, with war for example Taleb calculates it as casualties are bounded by the human population, and beyond some threshold like 50,000 thousand casualties, the conditonal mean is about 30,000,000 which is roughly 3x as much as is the case with the simple arithementic mean, therefore from a strict statistical point of view the world is possibly more dangerous than we think and violence may not be on the decline we might just be in a good period.

Given that we try to estimate an amount of uncertainity about the average of a distribution via the standard deviation / variance, it would only make sense to estimate some level of uncertainity about that uncertainity, and keeping constant with that train of thought and assuming we have no level of uncertainity about which we are certain then we can go infinitely deep, leading to an explosion of the second moment which creates very thick tails. So unless there is a decreasing level of uncertainity as we go out, or some point at which we are sure that's the amount of uncertainity we explode out effectively to power laws. Or as Taleb puts it, "Fat Tails are the Product of Recusrsive Epistemic Uncertainty".

Also in the same way that OTM options are convex to Std. Dev. as in as the Standard Deviation rises by 1 unit the value of the OTM option rises quicker than 1 unit and that rate accelerates the more Std. Dev. increases. They are also convex to uncertainity about the Standard Deviation, this is because as uncertainity about it rises it could be higher and so prices of OTM options rise. And in the same fashion the expectation or mean of a power law distributed variable is convex to the tail exponent and the uncertainity about the tail exponent. This convexity to uncertainity for some reason though appears to make the true mean assymetrically higher than lower than the in sample mean, leading to a further downward biased estimate of the expectation, but why and to what degree I'm entirely uncertain.

Given that we take p-values to be generated of a power law meta distribution, the probability of atleast one p-value in a series of expiriments being less than 0.05 is almost 100% even if the truth of the matter is weak or even no evidence against the Null Hypothesis, just due to the stochastic nature of an expiriment. The fixes suggested are looking for p-value < 0.005 or my preffered solution is to report the full distribution of p-values expirienced as the expirirment was done over and over as opposed to just reporting the minimum/statistically significant point p-value, so that it can be understood and fitting a distribution to it we can find the median p-value and MAD etc. to get a better idea of what's going on. Obviously stochasticity of the tail exponent introduuces further complications, but perhaps not a bad first step.

CLT states that your partial sum as Sn = A large enough number, the distribution will come out to being Gaussian. The Law of Large Numbers then says as the CLT kicks in your mean will end up concentrating and concentrating until you get a Dirac stick for Sn = A very large number i.e. low to no variance as n gets vry big. The problem is for power laws it takes a very very large n for those 2 things to kick in, assuming there even is a mean or a finite variance.

## With Repect to Work and Learning

It is not unreasonable to approach any obstacle of life or work as things to be tackled by a process of refined reasearch, with hypothesis, observations, analysis, and what that could possibly tell you about the domain with which you're interacting.

I find more and more that where things, fail, break, and don't break is substantially more important to understand then where they do. I suppose this is encapsulated in the 2 phrases, "It ain't what you know, it's what you know for sure but just aint so." and "I want to know where I'll die so I'll never go there". I think possibly just following those 2 ideas and constantly looking for where failure is likely and avoiding them leaves just the success to take care of itself.